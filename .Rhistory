verbal_span_mean = col_double(),
mean_blocks = col_double(),
interference_score = col_double(),
multiplyRT = col_double(),
multiplyCorr = col_double(),
productType = col_factor(),
problem = col_character()
)
combined_rt_df <- readr::read_csv("combined_mult_data.csv", col_types = col_types_spec) %>%
rename(visual_span_mean = mean_blocks) %>%
# Filter out incorrect trials
filter(multiplyCorr == 1) %>%
# Apply log10 transformation to reaction times
mutate(log10_rt = log10(multiplyRT)) %>%
# Apply effect coding/deviation coding
mutate(problem_size = ifelse(productType == "smallProd", -1, 1)) %>%
mutate(problem_size = as.factor(problem_size))
combined_acc_df <- readr::read_csv("combined_mult_data.csv", col_types = col_types_spec) %>%
rename(visual_span_mean = mean_blocks) %>%
# Apply log10 transformation to reaction times
mutate(log10_rt = log10(multiplyRT)) %>%
# Apply effect coding/deviation coding
mutate(problem_size = ifelse(productType == "smallProd", -1, 1)) %>%
mutate(problem_size = as.factor(problem_size))
# One more data frame for rate correct score (RCS)
View(combined_rt_df)
library(tidyverse)
library(lme4)
library(lmerTest)
library(emmeans)
library(sjPlot)
library(performance)
library(broom.mixed)
theme_set(theme_bw(base_size = 15) +
theme(legend.position="bottom",
panel.grid.major.x = element_blank()))
# Demographics data
demo_df <- read_csv("consolidated_data/combined_demo_data.csv") %>%
# Ensure "participant" column is character type
mutate(participant = as.character(participant))
# Typing, verbal & visual STM
combined_tasks <- read_csv("combined_mult_data.csv") %>%
mutate(participant = as.character(participant)) %>%
# Rename "mean_blocks" column to "visual_span_mean"
rename(visual_span_mean = mean_blocks)
# List of participants & save
participant_df <- data.frame(participant = combined_tasks$participant)
# write.csv(participant_df, "consolidated_data/participant_list.csv", row.names = FALSE)
View(combined_tasks)
# Group by participant and calculate the number of rows per participant
result_summary <- combined_tasks %>%
group_by(participant) %>%
dplyr::summarize(number_of_rows = n())
# Read the CSV file and assign it to combined_rt_df
## Check again if this dataset has excluded participants who didn't complete digit span, typing, vpt
missing_task_df <- readr::read_csv("combined_mult_data.csv") %>%
rename(visual_span_mean = mean_blocks) %>%
filter(multiplyCorr == 1) %>%
mutate(log10_rt = log10(multiplyRT)) %>%  # Log10 transformed the RT
dplyr::select(participant, verbal_span_mean, visual_span_mean, medium_mult_RT, multiplyRT) %>%
group_by(participant) %>%
summarize_all(~ sum(is.na(.))) %>%
filter_at(vars(-participant), any_vars(. != 0))
# Check the number of participants
n_participants_task_df <- readr::read_csv("combined_mult_data.csv") %>%
rename(visual_span_mean = mean_blocks) %>%
filter(multiplyCorr == 1) %>%
mutate(log10_rt = log10(multiplyRT)) %>%  # Log10 transformed the RT
dplyr::select(participant, verbal_span_mean, visual_span_mean, medium_mult_RT, multiplyRT) %>%
group_by(participant) %>%
summarise(n = n())
View(result_summary)
View(missing_task_df)
View(n_participants_task_df)
col_types_spec <- cols(
participant = col_factor(),
median_RT = col_double(),
verbal_span_mean = col_double(),
mean_blocks = col_double(),
interference_score = col_double(),
multiplyRT = col_double(),
multiplyCorr = col_double(),
productType = col_factor(),
problem = col_character()
)
combined_rt_df <- readr::read_csv("combined_mult_data.csv", col_types = col_types_spec) %>%
rename(visual_span_mean = mean_blocks) %>%
# Filter out incorrect trials
filter(multiplyCorr == 1) %>%
# Apply log10 transformation to reaction times
mutate(log10_rt = log10(multiplyRT)) %>%
# Apply effect coding/deviation coding
mutate(problem_size = ifelse(productType == "smallProd", -1, 1)) %>%
mutate(problem_size = as.factor(problem_size))
combined_acc_df <- readr::read_csv("combined_mult_data.csv", col_types = col_types_spec) %>%
rename(visual_span_mean = mean_blocks) %>%
# Apply log10 transformation to reaction times
mutate(log10_rt = log10(multiplyRT)) %>%
# Apply effect coding/deviation coding
mutate(problem_size = ifelse(productType == "smallProd", -1, 1)) %>%
mutate(problem_size = as.factor(problem_size))
# One more data frame for rate correct score (RCS)
View(combined_rt_df)
# using datawizard package, datawizard::standardize()
combined_rt_centered <- combined_rt_df %>%
mutate(participant = as.factor(participant)) %>%
mutate(problem_size = as.factor(problem_size)) %>%
mutate(interference_score_c = datawizard::center(interference_score),
verbal_span_c = datawizard::center(verbal_span_mean),
visual_span_c = datawizard::center(visual_span_mean),
typing_RT_c = datawizard::center(median_RT),
interference_score_z = datawizard::standardize(interference_score),
verbal_span_z = datawizard::standardize(verbal_span_mean),
visual_span_z = datawizard::standardize(visual_span_mean),
typing_RT_z = datawizard::standardize(median_RT))
combined_acc_centered <- combined_acc_df %>%
mutate(participant = as.factor(participant)) %>%
mutate(problem_size = as.factor(problem_size)) %>%
mutate(interference_score_c = datawizard::center(interference_score),
verbal_span_c = datawizard::center(verbal_span_mean),
visual_span_c = datawizard::center(visual_span_mean),
typing_RT_c = datawizard::center(median_RT),
interference_score_z = datawizard::standardize(interference_score),
verbal_span_z = datawizard::standardize(verbal_span_mean),
visual_span_z = datawizard::standardize(visual_span_mean),
typing_RT_z = datawizard::standardize(median_RT),
rt_z = datawizard::standardise(multiplyRT))
# Assuming 'multiplyCorr' is a numeric vector
combined_rt_centered$multiplyCorr <- factor(combined_rt_centered$multiplyCorr,
levels = c(0, 1),
labels = c("Wrong", "Correct"))
# problem size and interference
problem_size_numeric = as.numeric(combined_rt_centered$problem_size)
correlation_test <- cor.test(problem_size_numeric, combined_rt_centered$interference_score_z,
method = "pearson")
print(correlation_test)
# problem size and log10_rt
size_rt_corr_test <- cor.test(problem_size_numeric, combined_rt_centered$log10_rt, method = "pearson")
print(size_rt_corr_test)
# interference and log10_rt
int_rt_corr_test <- cor.test(combined_rt_centered$interference_score_z, combined_rt_centered$log10_rt, method = "pearson")
print(int_rt_corr_test)
m_0 <- lmer(log10_rt ~ typing_RT_z  +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
summary(m_0)
anova(m_0, type = "III") # Type III tests
performance::model_performance(m_0)
# Fit the model
m_1a <- lmer(log10_rt ~ typing_RT_z + problem_size  +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
summary(m_1a)
anova(m_1a, type = "III") # Type III tests
performance::model_performance(m_1a)
# Fit the model
m_2a <- lmer(log10_rt ~ typing_RT_z + problem_size + interference_score_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
summary(m_2a)
anova(m_2a, type = "III") # Type III tests
performance::model_performance(m_2a)
# Fit the model
m_3a <- lmer(log10_rt ~ typing_RT_z + problem_size + interference_score_z +
verbal_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
summary(m_3a)
anova(m_3a, type = "III") # Type III tests
performance::model_performance(m_3a)
# Fit the model
m_4a <- lmer(log10_rt ~ typing_RT_z + problem_size + interference_score_z +
verbal_span_z + visual_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
summary(m_4a)
anova(m_4a, type = "III") # Type III tests
performance::model_performance(m_4a)
anova(m_0, m_1a)
anova(m_1a, m_2a)
anova(m_2a, m_3a)
anova(m_3a, m_4a)
# Fit the model
m_5a <- lmer(log10_rt ~ typing_RT_z  + interference_score_z +
visual_span_z + problem_size * verbal_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model5_summary = summary(m_5a)
model5_summary
anova(m_5a, type = "III") # Type III tests
performance::model_performance(m_5a)
# Fit the model
m_6a <- lmer(log10_rt ~ typing_RT_z  + interference_score_z +
verbal_span_z + problem_size * visual_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model6_summary = summary(m_6a)
model6_summary
anova(m_6a, type = "III") # Type III tests
performance::model_performance(m_6a)
# Fit the model
m_7a <- lmer(log10_rt ~ typing_RT_z  + problem_size +
visual_span_z + interference_score_z * verbal_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model7_summary = summary(m_7a)
model7_summary
anova(m_7a, type = "III") # Type III tests
performance::model_performance(m_7a)
m_8a <- lmer(log10_rt ~ typing_RT_z  + problem_size +
verbal_span_z + interference_score_z * visual_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model8_summary = summary(m_8a)
model8_summary
anova(m_8a, type = "III") # Type III tests
performance::model_performance(m_8a)
m_9a <- lmer(log10_rt ~ typing_RT_z  +
interference_score_z * verbal_span_z +
problem_size * visual_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model9_summary = summary(m_9a)
model9_summary
anova(m_9a, type = "III") # Type III tests
performance::model_performance(m_9a)
m_10a <- lmer(log10_rt ~ typing_RT_z  +
problem_size * verbal_span_z +
interference_score_z * verbal_span_z +
problem_size * visual_span_z +
interference_score_z * visual_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model10_summary = summary(m_10a)
model10_summary
anova(m_10a, type = "III") # Type III tests
performance::model_performance(m_10a)
anova(m_9a, m_10a)
anova(m_4a, m_5a)
anova(m_4a, m_6a)
anova(m_4a, m_7a)
anova(m_4a, m_8a)
anova(m_4a, m_9a)
anova(m_4a, m_10a)
anova(m_4a, m_5a)
anova(m_4a, m_6a)
anova(m_4a, m_7a)
anova(m_4a, m_8a)
anova(m_4a, m_9a)
anova(m_4a, m_10a)
anova(m_6a, m_9a)
anova(m_7a, m_9a)
anova(m_9a, m_10a)
ggplot(data = combined_rt_centered, aes(x = verbal_span_z, y = interference_score_z, color = factor(problem_size))) +
geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
labs(title = "Interaction Plot of Verbal Span and Interference by Problem Size",
x = "Verbal Span (Z-score)",
y = "Interference Score (Z-score)",
color = "Problem Size") +
scale_color_manual(values = c("red", "blue"))  # Set color palette
library(emmeans)
inter_ar <- round(mean(combined_rt_centered$interference_score_z) + sd(combined_rt_centered$interference_score_z), 1)
inter_br <- round(mean(combined_rt_centered$interference_score_z) - sd(combined_rt_centered$interference_score_z), 1)
verbal_ar <- round(mean(combined_rt_centered$verbal_span_z) + sd(combined_rt_centered$verbal_span_z), 1)
verbal_br <- round(mean(combined_rt_centered$verbal_span_z) - sd(combined_rt_centered$verbal_span_z), 1)
inter_ar <- round(mean(combined_rt_centered$interference_score_z) + sd(combined_rt_centered$interference_score_z), 1)
inter_br <- round(mean(combined_rt_centered$interference_score_z) - sd(combined_rt_centered$interference_score_z), 1)
## Setting interference as predictor, verbal span as moderator
verbal_ar <- round(mean(combined_rt_centered$verbal_span_z) + sd(combined_rt_centered$verbal_span_z), 1)
verbal_br <- round(mean(combined_rt_centered$verbal_span_z) - sd(combined_rt_centered$verbal_span_z), 1)
## Specify the list of points (lower, middle, upper)
inter_list <- list(interference = c(inter_br, inter_ar))
verbal_list <- list(verbal = c(verbal_br, verbal_ar))
inter_verbal <- lmer(log10_rt ~ typing_RT_z + interference_score_z * verbal_span_z  +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
emm_v_i <- emmeans(m_9a, ~ verbal_span_z | interference_score_z,
at = verbal_list,
pbkrtest.limit = 13010,
lmerTest.limit = 13010)
emms_problem_size <- emmeans(m_5a, ~ verbal_span_z | problem_size,
pbkrtest.limit = 13010,
lmerTest.limit = 13010)
# View the EMMs
summary(emms_problem_size)
emm_options(pbkrtest.limit = 50000)
verbal_ar <- round(mean(combined_rt_centered$verbal_span_z) + sd(combined_rt_centered$verbal_span_z), 1)
verbal_br <- round(mean(combined_rt_centered$verbal_span_z) - sd(combined_rt_centered$verbal_span_z), 1)
verbal_list <- list(verbal = c(verbal_ar, verbal_br))
# get the estimates
emtrends(m_7a, ~ verbal_span_z, var = "interference_score_z", at = verbal_list)
# statistical test for slope difference
emtrends(
m_7a,
pairwise ~ verbal_span_z,
var = "interference_score_z",
at = verbal_list,
adjust = "none"
)
# use small as basline
combined_rt_centered$problem_size <- relevel(combined_rt_centered$problem_size, ref = "-1")
emtrends(m_6a, ~ problem_size, var = "visual_span_z")
# test difference in slopes
emtrends(m_6a, pairwise ~ problem_size, var = "visual_span_z")
# Fit the model
m_6a <- lmer(log10_rt ~ typing_RT_z  + interference_score_z +
verbal_span_z + problem_size * visual_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model6_summary = summary(m_6a)
model6_summary
anova(m_6a, type = "III") # Type III tests
performance::model_performance(m_6a)
## Post-hoc
# Create a data frame with the four scenarios
scenarios <- data.frame(
problem_size = c(-1, -1, 1, 1),
visual_span_z = c(1, -1, 1, -1)
)
# Use ggpredict to obtain predicted values for the scenarios
predictions <- ggpredict(m_6a, scenarios)
library(tidyverse)
library(lme4)
library(lmerTest)
library(emmeans)
library(sjPlot)
library(performance)
library(broom.mixed)
library(ggeffects)
theme_set(theme_bw(base_size = 15) +
theme(legend.position="bottom",
panel.grid.major.x = element_blank()))
# Fit the model
m_6a <- lmer(log10_rt ~ typing_RT_z  + interference_score_z +
verbal_span_z + problem_size * visual_span_z +
(problem_size + interference_score_z | participant) +
(1 | problem),
data = combined_rt_centered,
REML = FALSE,
control=lmerControl(optimizer="bobyqa",
optCtrl=list(maxfun=10000)))
# Summary and hypothesis tests for the model
model6_summary = summary(m_6a)
model6_summary
anova(m_6a, type = "III") # Type III tests
performance::model_performance(m_6a)
## Post-hoc
# Create a data frame with the four scenarios
scenarios <- data.frame(
problem_size = c(-1, -1, 1, 1),
visual_span_z = c(1, -1, 1, -1)
)
# Use ggpredict to obtain predicted values for the scenarios
predictions <- ggpredict(m_6a, scenarios)
plot(predictions)
# Create a summary data frame for connecting the lines
summary_data <- predictions %>%
group_by(x) %>%
arrange(x, group) %>%
mutate(visual_span_z = factor(group))
# Create the interaction plot with connected lines
interaction_visual_problem <- ggplot(data = summary_data, aes(x = factor(x), y = predicted, color = group)) +
geom_line(aes(group = group), size = 1) +
labs(
x = "Problem Size",
y = "Predicted log10 RT",
title = "Predicted log10 RT by Visual Span and Problem Size"
) +
scale_color_discrete(labels = c("Low Visual Span", "High Visual Span")) +
scale_y_continuous(breaks = seq(0.20, 0.40, by = 0.05)) +
theme_classic()
print(interaction_visual_problem)
plot(predictions)
# Save the plot as a high-resolution image
ggsave("interaction_visual_problem.png", plot = interaction_visual_problem, width = 8, height = 6, dpi = 600)
interaction_visual_problem <- ggplot(data = summary_data, aes(x = factor(x), y = predicted, color = group)) +
geom_line(aes(group = group), size = 1) +
labs(
x = "Problem Size",
y = "Predicted log10 RT",
title = "Predicted log10 RT by Visual Span and Problem Size"
) +
scale_color_discrete(labels = c("Low Visual Span", "High Visual Span")) +
scale_y_continuous(breaks = seq(0.20, 0.40, by = 0.05), limits = c(0.20, 0.40)) +
theme_classic()
print(interaction_visual_problem)
emm_options(pbkrtest.limit = 15000)
verbal_ar <- round(mean(combined_rt_centered$verbal_span_z) + sd(combined_rt_centered$verbal_span_z), 1)
verbal_br <- round(mean(combined_rt_centered$verbal_span_z) - sd(combined_rt_centered$verbal_span_z), 1)
verbal_list <- list(verbal = c(verbal_ar, verbal_br))
# get the estimates
emtrends(m_7a, ~ interference_score_z, var = "verbal_span_z", at = verbal_list)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# n is number of correct responses
## RCS is number of correct responses / average reaction time
combined_df <- read_csv("combined_tasks.csv") %>%
mutate (rcs_score = n/mean_mult_RT) %>%
mutate(fluency_group = ifelse(rcs_score <= quantile(rcs_score, 0.5), "Low", "High"))
interference_score <- read_csv("consolidated_data/stim_inteference_score.csv")
mult_rt <- read_csv("consolidated_data/cleaned_mult-acc_data.csv")  %>%
inner_join(combined_df, by = 'participant') %>%
dplyr::select(participant, multiplyCorr, multiplyRT,
problem, productType,
median_RT, perc_accuracy,
verbal_span_mean, mean_blocks,
medium_mult_RT, perc_accuracy_mult, fluency_group) %>%
left_join(interference_score, by = 'problem')
# Save the combined data frame to a CSV file
write_csv(mult_rt, "combined_mult_data.csv")
# T-test for 'multiplication task - RT'
ttest_mult <- t.test(mean_mult_RT ~ fluency_group, data = combined_df)
# T-test for 'multiplication task - Accuracy'
ttest_mult_n <- t.test(n ~ fluency_group, data = combined_df)
# T-test for 'typing task'
ttest_typing <- t.test(median_RT ~ fluency_group, data = combined_df)
# T-test for 'verbal span'
ttest_verbal <- t.test(verbal_span_mean ~ fluency_group, data = combined_df)
# T-test for 'visual span'
ttest_visual <- t.test(mean_blocks ~ fluency_group, data = combined_df)
print(ttest_mult)
print(ttest_mult_n)
## Group differences in multiplication tasks
mult_ttest <- combined_df %>%
group_by(fluency_group) %>%
dplyr::summarize(mult_mean = mean(mean_mult_RT),
mult_sd = sd(mean_mult_RT),
acc_mean = mean(n),
acc_sd = sd(n))
# Print the results
print(ttest_typing)
# Calculate mean and SD for variable1
combined_df_typing <- combined_df %>%
group_by(fluency_group) %>%
dplyr::summarize(n = n(),
fluency_mean = mean(median_RT),
fluency_sd = sd(median_RT))
print(ttest_verbal)
combined_df_verbal <- combined_df %>%
group_by(fluency_group) %>%
dplyr::summarize(fluency_mean = mean(verbal_span_mean),
fluency_sd = sd(verbal_span_mean))
print(ttest_visual)
combined_df_visual <- combined_df %>%
group_by(fluency_group) %>%
dplyr::summarize(fluency_mean = mean(mean_blocks),
fluency_sd = sd(mean_blocks))
View(mult_ttest)
