---
title: "ANALYIS_mixed-models"
output: html_document
date: "2023-08-16"
---

# Import library
```{r message=FALSE}

library(tidyverse)
library(lme4)
library(lmerTest)
library(emmeans)
library(sjPlot)
library(performance)
library(broom.mixed)

```

# Load datasets

```{r message=FALSE}

# Demographics data
demo_df <- read_csv("consolidated_data/combined_demo_data.csv") %>%
  # Ensure "participant" column is character type
  mutate(participant = as.character(participant))

# Typing, verbal & visual STM
combined_tasks <- read_csv("combined_tasks.csv") %>%
  mutate(participant = as.character(participant)) %>% 
  # Rename "mean_blocks" column to "visual_span_mean"
  rename(visual_span_mean = mean_blocks) %>%
  # Select specific columns
  select(participant, median_RT, verbal_span_mean, visual_span_mean, medium_mult_RT) 

# List of participants & save
participant_df <- data.frame(participant = combined_tasks$participant)
write.csv(participant_df, "consolidated_data/participant_list.csv", row.names = FALSE)

# Multiplication and OTHER tasks
interference_score <- read_csv("consolidated_data/stim_inteference_score.csv")

# Multiplication with RT or accuracy as response
mult_df <- read_csv("consolidated_data/cleaned_mult-acc_data.csv") %>%
  # Ensure "participant" column is character type
  mutate(participant = as.character(participant)) %>%
  # Perform an inner join with "interference_score" by the "problem" column
  inner_join(interference_score, by = 'problem')

```

# Combined tasks + multiplication + demographics datasets

```{r}
# Inner join the two data frames based on the "participant" column
result_df <- mult_df %>%
  inner_join(combined_tasks, by = 'participant') 

# Group by participant and calculate the number of rows per participant
result_summary <- result_df %>%
  group_by(participant) %>%
  summarize(number_of_rows = n())

# Save the combined data frame to a CSV file
# write_csv(result_df, "combined_mult_data.csv")

# Read the CSV file and assign it to combined_rt_df
## Check again if this dataset has excluded participants who didn't complete digit span, typing, vpt
missing_task_df <- readr::read_csv("combined_mult_data.csv") %>% 
  filter(multiplyCorr == 1) %>% 
  mutate(log10_rt = log10(multiplyRT)) %>%  # Log10 transformed the RT
  dplyr::select(participant, verbal_span_mean, visual_span_mean, medium_mult_RT, multiplyRT) %>% 
  group_by(participant) %>%
  summarize_all(~ sum(is.na(.))) %>% 
  filter_at(vars(-participant), any_vars(. != 0))

# Check the number of participants
n_participants_task_df <- readr::read_csv("combined_mult_data.csv") %>% 
  filter(multiplyCorr == 1) %>% 
  mutate(log10_rt = log10(multiplyRT)) %>%  # Log10 transformed the RT
  dplyr::select(participant, verbal_span_mean, visual_span_mean, medium_mult_RT, multiplyRT) %>% 
  group_by(participant) %>%
  summarise(n = n())
```

# Removing incorrect trials from the multiplication task for RT models (combined_rt_df)
# DO NOT remove incorrect trials for ACCURACY models (combined_acc_df)

```{r data_df}

col_types_spec <- cols(
  participant = col_factor(),
  median_RT = col_double(),
  verbal_span_mean = col_double(),
  visual_span_mean = col_double(),
  interference_score = col_double(),
  multiplyRT = col_double(),
  multiplyCorr = col_double(),
  productType = col_factor(),
  OperandType = col_factor(),
  problem = col_character()
)

combined_rt_df <- readr::read_csv("combined_mult_data.csv", col_types = col_types_spec) %>% 
  # Filter out incorrect trials
  filter(multiplyCorr == 1) %>% 
  # Apply log10 transformation to reaction times
  mutate(log10_rt = log10(multiplyRT)) %>% 
  # Apply effect coding/deviation coding
  mutate(problem_size = ifelse(productType == "smallProd", -1, 1)) %>% 
  mutate(problem_size = as.factor(problem_size))

combined_acc_df <- readr::read_csv("combined_mult_data.csv", col_types = col_types_spec) %>% 
  # Apply log10 transformation to reaction times
  mutate(log10_rt = log10(multiplyRT)) %>% 
  # Apply effect coding/deviation coding
  mutate(problem_size = ifelse(productType == "smallProd", -1, 1)) %>% 
  mutate(problem_size = as.factor(problem_size))

# One more data frame for rate correct score (RCS)

```

# Centering and standardize the predictors for verbal and visual span means

```{r}

# using datawizard package, datawizard::standardize()

combined_rt_centered <- combined_rt_df %>% 
  mutate(participant = as.factor(participant)) %>% 
  mutate(problem_size = as.factor(problem_size)) %>% 
  mutate(interference_score_c = datawizard::center(interference_score),
        verbal_span_c = datawizard::center(verbal_span_mean),
         visual_span_c = datawizard::center(visual_span_mean),
         typing_RT_c = datawizard::center(median_RT),
        interference_score_z = datawizard::standardize(interference_score),
        verbal_span_z = datawizard::standardize(verbal_span_mean),
        visual_span_z = datawizard::standardize(visual_span_mean),
        typing_RT_z = datawizard::standardize(median_RT))

combined_acc_centered <- combined_acc_df %>% 
  mutate(participant = as.factor(participant)) %>% 
  mutate(problem_size = as.factor(problem_size)) %>% 
  mutate(interference_score_c = datawizard::center(interference_score),
         verbal_span_c = datawizard::center(verbal_span_mean),
         visual_span_c = datawizard::center(visual_span_mean),
         typing_RT_c = datawizard::center(median_RT),
         interference_score_z = datawizard::standardize(interference_score),
         verbal_span_z = datawizard::standardize(verbal_span_mean),
        visual_span_z = datawizard::standardize(visual_span_mean),
         typing_RT_z = datawizard::standardize(median_RT),
        rt_z = datawizard::standardise(multiplyRT))

# Assuming 'multiplyCorr' is a numeric vector
combined_rt_centered$multiplyCorr <- factor(combined_rt_centered$multiplyCorr, 
                                            levels = c(0, 1), 
                                            labels = c("Wrong", "Correct"))


```


## ----------------- RANDOM STRUCTURE -------------------------------------

```{r}

# KEEP IT MAXIMAL
# If singular, use rePCA and VarCorr

rs_1a <- lmer(log10_rt ~ problem_size * interference_score_c * verbal_span_c * visual_span_c +
               typing_RT_c +
               (problem_size * interference_score_c | participant) + (1 | problem), 
             data = combined_rt_centered)

summary(rePCA(rs_1a))
VarCorr(rs_1a)

# only need five components in random structure
```

### Zero-correlation-parameter

```{r}

rs_1b <- lmer(log10_rt ~ problem_size * interference_score_c * verbal_span_c * visual_span_c +
               typing_RT_c +
               (problem_size * interference_score_c || participant) + (1 | problem), 
             data = combined_rt_centered,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

summary(rePCA(rs_1b))
VarCorr(rs_1b)

# remove interactions
```

```{r}
rs_1c <- lmer(log10_rt ~ problem_size * interference_score_c * verbal_span_c * visual_span_c +
               typing_RT_c +
               (problem_size + interference_score_c || participant) + (1 | problem), 
             data = combined_rt_centered, 
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

summary(rePCA(rs_1c))
VarCorr(rs_1c)

# high correlation between problem_size and participant
```


```{r}
rs_1d <- lmer(log10_rt ~ problem_size * interference_score_c * verbal_span_c * visual_span_c + 
                typing_RT_c +
               (problem_size + interference_score_c | participant) + (1 | problem), 
             data = combined_rt_centered, 
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

summary(rePCA(rs_1d))
VarCorr(rs_1d)

# large eigenvalues, high correlation, failed to converge
## Okay! this is the best random slope
```

```{r}
rs_1e <- lmer(log10_rt ~ problem_size * interference_score_c * verbal_span_c * visual_span_c +
               typing_RT_c +
               (problem_size | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

summary(rePCA(rs_1e))
VarCorr(rs_1e)

# OKAY!
```


```{r}
rs_1f <- lmer(log10_rt ~ problem_size * interference_score_c * verbal_span_c * visual_span_c +
               typing_RT_c +
               (interference_score_c | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

summary(rePCA(rs_1f))
VarCorr(rs_1f)

# OKAY!
```

```{r}
rs_1g <- lmer(log10_rt ~ problem_size * interference_score_z * verbal_span_z * visual_span_z +
               typing_RT_z +
               (problem_size + interference_score_z | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

summary(rePCA(rs_1g))
VarCorr(rs_1g)
```


# Compare the three random slope models
```{r}

performance::compare_performance(rs_1d, rs_1e, rs_1f, rank = TRUE)
```


### Finding best optimizer for rs_1d
```{r}
best_optimizer <- allFit(rs_1d) 

glance(best_optimizer) |> select(optimizer, AIC, NLL_rel) |> 
    arrange(NLL_rel)  

tidy(best_optimizer, conf.int = TRUE) |> 
     arrange(effect, term, estimate) |> 
     select(-c(std.error, statistic))


```

# The best optimizer="bobyqa"

## ----------------- MAIN ANALYSIS -------------------------------------

# Hierachical multilevel modelling
## Model 0- baseline

```{r}

m_0 <- lmer(log10_rt ~ typing_RT_z  +
               (problem_size + interference_score_z | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             REML = FALSE,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

# Summary and hypothesis tests for the model
summary(m_0)
anova(m_0, type = "III") # Type III tests
performance::model_performance(m_0)
```




## Model 1 (problem features - size -1 is small, 1 is large)
## Model 1a  (problem size)

```{r}
# Fit the model
m_1a <- lmer(log10_rt ~ typing_RT_z + problem_size  +
               (problem_size + interference_score_z | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             REML = FALSE,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

# Summary and hypothesis tests for the model
summary(m_1a)
anova(m_1a, type = "III") # Type III tests
performance::model_performance(m_1a)
```


## Model 2 (problem features - interference)
## Model 2a with interference score

```{r}
# Fit the model
m_2a <- lmer(log10_rt ~ typing_RT_z + problem_size + interference_score_z +
               (problem_size + interference_score_z | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             REML = FALSE,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

# Summary and hypothesis tests for the model
summary(m_2a)
anova(m_2a, type = "III") # Type III tests

performance::model_performance(m_2a)
```


# Model 3 (Verbal STM)
## Verbal Span Mean

```{r}
# Fit the model
m_3a <- lmer(log10_rt ~ typing_RT_z + problem_size + interference_score_z +
               verbal_span_z + 
               (problem_size + interference_score_z | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             REML = FALSE,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))

# Summary and hypothesis tests for the model
summary(m_3a)
anova(m_3a, type = "III") # Type III tests

performance::model_performance(m_3a)
```

# Model 4 (Visual STM)
## Model 4a and 4b (visual_span_mean)

```{r}
# Fit the model
m_4a <- lmer(log10_rt ~ typing_RT_z + problem_size + interference_score_z +
               verbal_span_z + visual_span_z +
               (problem_size + interference_score_z | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             REML = FALSE,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))


# Summary and hypothesis tests for the model
summary(m_4a)
anova(m_4a, type = "III") # Type III tests

performance::model_performance(m_4a)
```

# Model 5 - interactions

```{r}
# Fit the model
m_5a <- lmer(log10_rt ~ typing_RT_z + problem_size * interference_score_z *
               verbal_span_z * visual_span_z +
               (problem_size + interference_score_z | participant) + 
                (1 | problem), 
             data = combined_rt_centered,
             REML = FALSE,
             control=lmerControl(optimizer="bobyqa", 
                                 optCtrl=list(maxfun=10000)))



# Summary and hypothesis tests for the model
model5_summary = summary(m_5a)
model5_summary
anova(m_5a, type = "III") # Type III tests

performance::model_performance(m_5a)
```


```{r}
anova(m_5a, type = "II")
```


```{r}

# Obtain the coefficient estimates
coef_estimates <- fixef(m_1a) # Fixed-effects coefficients
# Apply the exp() function to convert to exp values
exp_estimates <- exp(coef_estimates)
exp_estimates

```


```{r}
print(summary(m_5a,ddf="Satterthwaite"),correlation=FALSE)
```

## Compare model 4 and 5

```{r}
anova(m_1a, m_2a)
anova(m_2a, m_3a)
anova(m_3a, m_4a)

anova(m_4a, m_5a)
anova(m_3a, m_5a)

anova(m_1a, m_5a)
anova(m_2a, m_5a)
anova(m_3a, m_5a)
anova(m_4a, m_5a)

```


```{r}
library(sjPlot)
tab_model(m_1a, m_2a, m_3a, m_4a, m_5a)
```


## Post hoc
### Interactions between interference and verbal span

```{r}
# Example predictor values
interference_values <- c(-2, -1, 0, 1, 2)
verbal_span_values <- c(-2, -1, 0, 1, 2)
visual_span_values <- c(-2, -1, 0, 1, 2)

# Create a data frame with combinations of predictor values
predictors_df <- expand.grid(
  interference_score_z = interference_values,
  verbal_span_z = verbal_span_values,
  visual_span_z = visual_span_values,
  problem_size = c(-1, 1)  # Example problem_size values
)

# Set the constant value for typing_RT_z
constant_typing_RT_z <- 0  # You can set it to any constant value you prefer

# Include 'typing_RT_z' in 'predictors_df' with the constant value
predictors_df$typing_RT_z <- constant_typing_RT_z

# Convert potential factors to numeric
predictors_df$interference_score_z <- as.numeric(predictors_df$interference_score_z)
predictors_df$verbal_span_z <- as.numeric(predictors_df$verbal_span_z)
predictors_df$visual_span_z <- as.numeric(predictors_df$visual_span_z)
predictors_df$problem_size <- as.numeric(predictors_df$problem_size)

# Prediction
predictions <- predict(m_5a, newdata = predictors_df, re.form = ~0)



```

### Emmeans
## Continuous predictors
### verbal span as predictor and interference as moderator on predicting reaction time.
# Question: Is there a difference in the relationship of verbal STM on RT for different values of interference score? (comparing simple slopes)

```{r}
library(emmeans)


```
```{r}
library(ggplot2)

ggplot(data = combined_rt_centered, aes(x = interference_score, y = log10_rt, color = factor(verbal_span_mean))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = verbal_span_mean), formula = y ~ x) +
  labs(x = "Interference Score", y = "Log10_RT", color = "Verbal Span Mean") +
  facet_wrap(~verbal_span_mean, scales = "free") +
  theme_minimal()

```


```{r}
library(ggplot2)


ggplot(data = combined_rt_centered, aes(x = interference_score, y = log10_rt, color = verbal_span_mean, group = factor(verbal_span_mean))) +
  geom_point(position = position_jitter(width = 0.1, height = 0), size = 1) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  labs(x = "Interference Score", y = "Log10_RT", color = "Verbal Span Mean") +
  scale_color_gradient(low = "lightblue", high = "blue") +
  theme_minimal()


```

```{r}
ggplot(data = combined_rt_centered, aes(x = visual_span_mean, y = log10_rt, color = factor(productType))) +
  geom_point(position = position_jitter(width = 0.1, height = 0), size = 0.5) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  labs(x = "Visual Span Mean", y = "Log10_RT", color = "Problem Size") +
  scale_color_manual(values = c("smallProd" = "lightblue", "largeProd" = "blue")) +
  theme_minimal()

```


```{r}
ggplot(data = combined_rt_centered, aes(x = verbal_span_z, y = log10_rt, color = visual_span_z, group = factor(visual_span_z))) +
  geom_point(position = position_jitter(width = 0.1, height = 0), size = 1) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  labs(x = "Verbal Span Mean", y = "Log10_RT", color = "Visual Span Mean") +
  scale_color_gradient(low = "lightblue", high = "blue") +
  theme_minimal()
```


## ----------------------Accuracy --------------------------------------
## Accuracy model

## -------------------RANDOM STRUCTURE FOR ACCURACY ------------------

# Speed-accuracy tradeoff (SAT)
refers to 
```{r}

# Baseline Model (Level 0)
baseline_model <- glmer(multiplyCorr ~ log10_rt + typing_RT_z +
                          (log10_rt || problem), 
                        data = combined_acc_centered, 
                        family = binomial(link = "logit"),
                        control = glmerControl(optCtrl = list(maxfun = 20000)))



summary(rePCA(baseline_model))
VarCorr(baseline_model)
summary(baseline_model)
performance::model_performance(baseline_model)
```


## --------------------- MAIN ANALYSIS FOR ACCURACY ---------------------

## Correlation between accuracy and RT

```{r}

corr_data <- combined_acc_centered %>% 
  select(multiplyRT, multiplyCorr, verbal_span_z, 
         visual_span_z, interference_score_z, problem_size) %>% 
  mutate(problem_size = as.numeric(problem_size))

pairs(corr_data)

library(correlation)

correlation::correlation(corr_data,
  include_factors = TRUE, method = "auto"
)
```



## CONTROL for reaction time in each trial and typing speed

```{r}
# Baseline Model (Level 0)
baseline_model <- glmer(multiplyCorr ~ log10_rt + typing_RT_z +
                          (log10_rt || problem), 
                        data = combined_acc_centered, 
                        family = binomial(link = "logit"),
                        control = glmerControl(optCtrl = list(maxfun = 20000)))

summary(baseline_model)
performance::model_performance(baseline_model)
```

## Problem size

```{r}

acc_model_1 <- glmer(multiplyCorr ~ log10_rt + typing_RT_z + problem_size +
                          (log10_rt || problem), 
                        data = combined_acc_centered, 
                        family = binomial(link = "logit"),
                        control = glmerControl(optCtrl = list(maxfun = 20000)))

summary(acc_model_1)


# Calculate odd ratios
exp(fixef(acc_model_1))

# calculate CI
exp(confint(acc_model_1))


performance::model_performance(acc_model_1)
```


## Interference score
```{r}

acc_model_2 <- glmer(multiplyCorr ~ log10_rt + typing_RT_z + problem_size +
                       interference_score_z +
                          (log10_rt || problem), 
                        data = combined_acc_centered, 
                        family = binomial(link = "logit"),
                        control = glmerControl(optCtrl = list(maxfun = 20000)))

summary(acc_model_2)

# Calculate odd ratios
exp(fixef(acc_model_2))

# calculate CI
exp(confint(acc_model_2))



performance::model_performance(acc_model_2)
```

## verbal stm

```{r}

acc_model_3 <- glmer(multiplyCorr ~ log10_rt + typing_RT_z + problem_size +
                       interference_score_z + verbal_span_z +
                          (log10_rt || problem), 
                        data = combined_acc_centered, 
                        family = binomial(link = "logit"),
                        control = glmerControl(optCtrl = list(maxfun = 20000)))

summary(acc_model_3)

# Calculate odd ratios
exp(fixef(acc_model_3))

# calculate CI
exp(confint(acc_model_3))

performance::model_performance(acc_model_3)
```

## Visual stm

```{r}

acc_model_4 <- glmer(multiplyCorr ~ log10_rt + typing_RT_z + problem_size +
                       interference_score_z + verbal_span_z + visual_span_z +
                          (log10_rt || problem), 
                        data = combined_acc_centered, 
                        family = binomial(link = "logit"),
                        control = glmerControl(optCtrl = list(maxfun = 20000)))

summary(acc_model_4)
# Calculate odd ratios
exp(fixef(acc_model_4))

# calculate CI
exp(confint(acc_model_4))


performance::model_performance(acc_model_4)
```

## Interactions (model failed to converge)

```{r}
acc_model_5 <- glmer(multiplyCorr ~ log10_rt + typing_RT_z + problem_size *
                       interference_score_z * verbal_span_z * visual_span_z +
                          (log10_rt || problem), 
                        data = combined_acc_centered, 
                        family = binomial(link = "logit"),
                        control = glmerControl(optCtrl = list(maxfun = 20000)))

summary(acc_model_5)
exp(fixef(acc_model_5))

performance::model_performance(acc_model_5)
```


## Compare models

```{r}
tab_model(baseline_model, acc_model_1, acc_model_2, acc_model_3, acc_model_4)
```


```{r}
compare_performance(baseline_model, acc_model_1, acc_model_2, acc_model_3, acc_model_4, rank = TRUE)

tab_model(baseline_model, acc_model_3, p.val = "kr", show.df = TRUE)
```

